{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17772,
     "status": "ok",
     "timestamp": 1645946248457,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "UbCZL7QoKvEl",
    "outputId": "ef77d2f3-e293-4185-f250-5f0275b9661f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22579,
     "status": "ok",
     "timestamp": 1645946273486,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "L00_cCVVzdk6",
    "outputId": "bce4c606-e972-424a-f858-177eba6db7d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from albumentations) (1.8.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from albumentations) (0.19.2)\n",
      "Collecting numpy>=1.11.1\n",
      "  Using cached numpy-1.22.2-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: PyYAML in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from albumentations) (4.1.2.30)\n",
      "Requirement already satisfied: typing-extensions in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: imageio>=2.4.1 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.16.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2022.2.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: networkx>=2.2 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.22.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "d2l 0.17.3 requires matplotlib==3.3.3, which is not installed.\n",
      "d2l 0.17.3 requires numpy==1.18.5, but you have numpy 1.22.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless==4.1.2.30 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (4.1.2.30)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from opencv-python-headless==4.1.2.30) (1.22.2)\n",
      "Collecting numpy==1.19.3\n",
      "  Using cached numpy-1.19.3-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.2\n",
      "    Uninstalling numpy-1.22.2:\n",
      "      Successfully uninstalled numpy-1.22.2\n",
      "Successfully installed numpy-1.19.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "d2l 0.17.3 requires matplotlib==3.3.3, which is not installed.\n",
      "imageio 2.16.1 requires numpy>=1.20.0, but you have numpy 1.19.3 which is incompatible.\n",
      "d2l 0.17.3 requires numpy==1.18.5, but you have numpy 1.19.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.1-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (1.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in d:\\development_tools\\anaconda3\\envs\\limupytorch\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: matplotlib\n",
      "Successfully installed matplotlib-3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "d2l 0.17.3 requires matplotlib==3.3.3, but you have matplotlib 3.5.1 which is incompatible.\n",
      "d2l 0.17.3 requires numpy==1.18.5, but you have numpy 1.19.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations\n",
    "!pip install opencv-python-headless==4.1.2.30 \n",
    "!pip install numpy==1.19.3\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1645946273488,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "dxMHSprcpLaj",
    "outputId": "eb441553-fbe6-44fa-c381-4636509d9324",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  1 11:10:47 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   53C    P8     4W /  N/A |    340MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1680    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7679,
     "status": "ok",
     "timestamp": 1645946286061,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "658CWSNiNC3G"
   },
   "outputs": [],
   "source": [
    "# 作者:ZhaoYu Wang\n",
    "# 日期:2022年02月22日\n",
    "import os\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import matplotlib  \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models.detection\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "ROOT_DIR = \"\"\n",
    "\n",
    "IMAGE_SIZE = 900\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1645946296991,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "sCnBQgk1DQ1J"
   },
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    direciton_list = os.listdir(path)\n",
    "#     print(direciton_list)\n",
    "#     print(len(direciton_list))\n",
    "    image_shape_list = []\n",
    "    print(\"reading images......\")\n",
    "    for imgfile in direciton_list:\n",
    "      image_path = path + imgfile\n",
    "      # print(image_path)\n",
    "      img = cv2.imread(image_path)\n",
    "      # print(imgfile)\n",
    "      img_shape = (img.shape[0], img.shape[1])\n",
    "      images.append(img)\n",
    "      image_paths.append(image_path)\n",
    "\n",
    "      del img\n",
    "      image_shape_list.append(img_shape)\n",
    "\n",
    "    image_ids = [i for i in range(len(images))]\n",
    "    print(\"reading done\")\n",
    "    return image_paths, image_ids, image_shape_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1645946299697,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "Wude8ZX8DWvc"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None):\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.length = len(dataframe)\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        def get_boxes(path, img_shape):\n",
    "            # print(img_shape)\n",
    "            boxes = []\n",
    "            label_path = ROOT_DIR + \"labels/\" + path.split('/')[-1].split('.')[0] + \".txt\"\n",
    "            # print(label_path)\n",
    "            text_file = open(label_path, \"r\")\n",
    "            for line in text_file:\n",
    "                info_list = line.split(\",\")[:9]\n",
    "                # print(info_list)\n",
    "                x1, y1, x2, y2, x3, y3, x4, y4 = map(int, info_list[:8])\n",
    "                if max(y1, y2, y3, y4)>600:\n",
    "                  print(path)\n",
    "                x_min = min(x1, x2, x3, x4) * (IMAGE_SIZE / img_shape[1])\n",
    "                x_max = max(x1, x2, x3, x4) * (IMAGE_SIZE / img_shape[1])\n",
    "                y_min = min(y1, y2, y3, y4) * (IMAGE_SIZE / img_shape[0])\n",
    "                y_max = max(y1, y2, y3, y4) * (IMAGE_SIZE / img_shape[0])\n",
    "                # if 600*1.205 == y_max:\n",
    "                #   print(path)\n",
    "                # print([x_min, y_min, x_max, y_max])\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "            return np.array(boxes)\n",
    "\n",
    "        image_id = self.image_ids[item]\n",
    "        path = self.dataframe.iloc[item].path\n",
    "\n",
    "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255\n",
    "\n",
    "        # plt.imshow(image)\n",
    "        # plt.show()\n",
    "\n",
    "        boxes = get_boxes(path, self.dataframe.iloc[item].image_shape)\n",
    "        # print(boxes)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([item])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id,path\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1645946302754,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "Y1DdI2phDbs8"
   },
   "outputs": [],
   "source": [
    "class TestTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None):\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.length = len(dataframe)\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        image_id = self.image_ids[item]\n",
    "        path = self.dataframe.iloc[item].path\n",
    "\n",
    "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "        return image, image_id,path\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1645946305323,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "qOfQaa7tDe8a"
   },
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1645946308867,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "fgSN1NMFDhsF"
   },
   "outputs": [],
   "source": [
    "# Albumentations\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_test_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ErLRwGRYDkm0"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# image_paths, image_ids, image_shape_list = load_images(ROOT_DIR + \"/images/\")\n",
    "# dataframe = pd.DataFrame(np.array(image_paths), columns=['path'])\n",
    "# dataframe['image_id'] = image_ids\n",
    "# dataframe['image_shape'] = image_shape_list\n",
    "# # print(dataframe.head())\n",
    "# print(dataframe['image_id'].unique().shape[0])  # 626\n",
    "\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# num_classes = 2  # 1 class (wheat) + background\n",
    "# # get number of input features for the classifier\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# # replace the pre-trained head with a new one\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# # 划分训练集和验证集\n",
    "# split_point = int(dataframe['image_id'].unique().shape[0] * 0.75)\n",
    "# # print(\"划分训练集和验证集点\", split_point)\n",
    "\n",
    "# train_df = dataframe.iloc[:split_point]\n",
    "# # valid_df = dataframe.iloc[split_point:-3]\n",
    "# valid_df = dataframe.iloc[split_point:]\n",
    "# # test_df = dataframe.iloc[-3:]\n",
    "\n",
    "# train_dataset = TextDataset(train_df, get_train_transform())\n",
    "# valid_dataset = TextDataset(valid_df, get_valid_transform())\n",
    "# test_dataset = TextDataset(test_df, get_valid_transform())\n",
    "\n",
    "# # split the dataset in train and test set\n",
    "# indices = torch.randperm(len(train_dataset)).tolist()\n",
    "# train_data_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=16,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )\n",
    "# valid_data_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )\n",
    "# test_data_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# images, targets, image_ids ,images_paths= next(iter(train_data_loader))\n",
    "# images = list(image.to(device) for image in images)\n",
    "# targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "# boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "# # print(type(images[2]))\n",
    "# sample = images[2].permute(1,2,0).cpu().numpy()\n",
    "# # print(sample)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1)\n",
    "# # ax.imshow(sample)\n",
    "# for box in boxes:\n",
    "#     cv2.rectangle(sample,\n",
    "#                   (box[0], box[1]),\n",
    "#                   (box[2], box[3]),\n",
    "#                   (220, 0, 0), 1)\n",
    "# # plt.imshow(sample)\n",
    "# # ax.imshow(sample)\n",
    "# # plt.savefig('pic1.jpg', bbox_inches='tight', dpi=1000)\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model.to(device)\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer  = torch.optim.SGD(params,lr = 0.005,momentum=0.9,weight_decay=0.0005)\n",
    "# lr_scheduler=None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faster_rcnn_model_v2(model):\n",
    "    new_num_class = 10\n",
    "    model_new = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        num_classes=new_num_class)\n",
    "    \n",
    "    pretrained_state_dict = model.state_dict()\n",
    "    change_keys = [\n",
    "        'roi_heads.box_predictor.cls_score.weight',\n",
    "        'roi_heads.box_predictor.cls_score.bias',\n",
    "        'roi_heads.box_predictor.bbox_pred.weight',\n",
    "        'roi_heads.box_predictor.bbox_pred.bias'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    from_scratch_state_dict = model_new.state_dict()\n",
    "    \n",
    "   \n",
    "    for change_key in change_keys:\n",
    "        pretrained_state_dict[change_key] = from_scratch_state_dict[change_key]\n",
    "    \n",
    "    model_new.load_state_dict(pretrained_state_dict)\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  1 11:11:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8     4W /  N/A |    340MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1680    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35156,
     "status": "ok",
     "timestamp": 1645946353588,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "Ow8gBnN-ZNQs",
    "outputId": "f723d9e5-1ad3-4813-9466-effa976c7b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading images......\n",
      "reading done\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#预训练\n",
    "torch.cuda.empty_cache()\n",
    "image_paths, image_ids, image_shape_list = load_images(ROOT_DIR + \"images/\")\n",
    "dataframe = pd.DataFrame(np.array(image_paths), columns=['path'])\n",
    "dataframe['image_id'] = image_ids\n",
    "dataframe['image_shape'] = image_shape_list\n",
    "# print(dataframe.head())\n",
    "print(dataframe['image_id'].unique().shape[0])  # 626\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "num_classes = 2  # 1 class (wheat) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_fpn_epoch_3.pth'))\n",
    "\n",
    "# model = get_faster_rcnn_model_v2(model)\n",
    "\n",
    "\n",
    "# 划分训练集和验证集\n",
    "split_point = int(dataframe['image_id'].unique().shape[0] * 0.75)\n",
    "train_df = dataframe.iloc[:split_point]\n",
    "# valid_df = dataframe.iloc[split_point:-3]\n",
    "valid_df = dataframe.iloc[split_point:-3]\n",
    "test_df = dataframe.iloc[-3:]\n",
    "\n",
    "train_dataset = TextDataset(train_df, get_train_transform())\n",
    "valid_dataset = TextDataset(valid_df, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "images, targets, image_ids ,images_paths= next(iter(train_data_loader))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "boxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\n",
    "#print(type(images[1]))\n",
    "sample = images[1].permute(1,2,0).cpu().numpy()\n",
    "# print(sample)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "# ax.imshow(sample)\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 1)\n",
    "# plt.imshow(sample)\n",
    "# ax.imshow(sample)\n",
    "# plt.savefig('pic1.jpg', bbox_inches='tight', dpi=1000)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer  = torch.optim.SGD(params,lr = 0.005,momentum=0.9,weight_decay=0.0005)\n",
    "lr_scheduler=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\development_tools\\Anaconda3\\envs\\limuPytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20112/1943998057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\development_tools\\Anaconda3\\envs\\limuPytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\development_tools\\Anaconda3\\envs\\limuPytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "loss_hist = Averager()\n",
    "valid_loss_hist = Averager()\n",
    "itr = 1\n",
    "loss_list = []\n",
    "valid_loss_list = []\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    valid_loss_hist.reset()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #Train\n",
    "    model.train()\n",
    "    for images ,targets,image_ids,paths in train_data_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        images  = list(image.float().to(device) for image in images)\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images,targets)\n",
    "\n",
    "        losses  =sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 631324,
     "status": "error",
     "timestamp": 1645945692918,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "jIPiomU0mNMa",
    "outputId": "81fef97f-4dae-4800-dc20-50ed2e4e6f4b"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-876c48902a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "loss_hist = Averager()\n",
    "valid_loss_hist = Averager()\n",
    "itr = 1\n",
    "loss_list = []\n",
    "valid_loss_list = []\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    valid_loss_hist.reset()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #Train\n",
    "    model.train()\n",
    "    for images ,targets,image_ids,paths in train_data_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        images  = list(image.float().to(device) for image in images)\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images,targets)\n",
    "\n",
    "        losses  =sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        itr+=1\n",
    "\n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss:{loss_hist.value}\")\n",
    "    loss_list.append(loss_hist.value)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    for images ,targets,image_ids ,image_paths in valid_data_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        images =list(image.to(device) for image in images)\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images)\n",
    "            for x in range(len(loss_dict)):\n",
    "                if len(loss_dict[x]['scores'])>0:\n",
    "                    losses = ((sum((1-loss)**2 for loss in loss_dict[x]['scores'])))/len(loss_dict[x]['scores'])\n",
    "                    loss_value = losses.item()\n",
    "\n",
    "                    valid_loss_hist.send(loss_value)\n",
    "    print(f\"Epoch #{epoch} val_loss: {valid_loss_hist.value}\")\n",
    "    valid_loss_list.append(valid_loss_hist.value)\n",
    "    torch.save(model.state_dict(), f'models/fasterrcnn_resnet50_fpn_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1645946375018,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "TVk_pyvazjXA",
    "outputId": "8c57409b-bdb9-448a-8a26-f3676309a657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 28 22:32:52 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   41C    P0    27W /  70W |   4282MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "error",
     "timestamp": 1645944486184,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "vAWT0uM1FFwF",
    "outputId": "abf6bb97-fe72-488d-825c-7cd01a7c6522"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0551ad99e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "figs, axes = plt.subplots(1, 1, figsize=(20, 5))\n",
    "axes.plot(range(50), loss_list, 'r-', label='Train Loss')\n",
    "axes.plot(range(50), valid_loss_list, 'b-', label='Val Loss')\n",
    "axes.legend()\n",
    "axes.set_xlabel(\"Epoch\")\n",
    "axes.set_ylabel(\"Loss\")\n",
    "\n",
    "plt.show()\n",
    "figs.savefig('loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1645946380202,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "QQwASxmyCXXS"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as torchtrans\n",
    "from matplotlib import patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1645946381806,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "_FQ4gToGCOdq"
   },
   "outputs": [],
   "source": [
    "def plot_img_bbox(img, target):\n",
    "    # plot the image and bboxes\n",
    "    # Bounding boxes are defined as follows: x-min y-min width height\n",
    "    fig,a =plt.subplots(1,1)\n",
    "    fig.set_size_inches(5,5)\n",
    "    a.imshow(img)\n",
    "    for box in (target['boxes']):\n",
    "        x,y ,width,height = box[0],box[1],box[2]-box[0],box[3]-box[1]\n",
    "        rect = patches.Rectangle((x,y),\n",
    "                                 width,height,\n",
    "                                 linewidth=1,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none'\n",
    "                                 )\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "def torch_to_pil(img):\n",
    "    return torchtrans.ToPILImage()(img).convert('RGB')\n",
    "\n",
    "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'],orig_prediction['scores'],iou_thresh)\n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HIERYIs7dGc"
   },
   "outputs": [],
   "source": [
    "images, targets, image_ids ,image_paths= next(iter(valid_data_loader))\n",
    "\n",
    "images = list(img.to(device) for img in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[2].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# <================ Non max suppression =======================>\n",
    "\n",
    "indices = torch.randperm(len(valid_dataset)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(valid_dataset, indices[-10:])\n",
    "ig, tg, ig_id ,ig_path= dataset_test[5]\n",
    "with torch.no_grad():\n",
    "  \n",
    "    prediction = model([ig.to(device)])[0]\n",
    "\n",
    "# print(\"1111\")\n",
    "plot_img_bbox(torch_to_pil(ig), tg)\n",
    "# print(\"3333\")\n",
    "# print(prediction)\n",
    "plot_img_bbox(torch_to_pil(ig), prediction)\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "nms_prediction = apply_nms(prediction, iou_thresh=0.1)\n",
    "print('NMS APPLIED MODEL OUTPUT')\n",
    "\n",
    "# print(nms_prediction)\n",
    "plot_img_bbox(torch_to_pil(ig), nms_prediction)\n",
    "\n",
    "# <================ Non max suppression =======================>\n",
    "\n",
    "\n",
    "outputs = model(images)\n",
    "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1645946395054,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "rKmQL4hZrj_L",
    "outputId": "7bc2d63c-1ae6-4a90-ab46-9b46acbd78ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1645946394.6290905"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1645946393787,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "3rXa5RS7Si5Q"
   },
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return pred_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1645946399924,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "GvSCmwURSx2A"
   },
   "outputs": [],
   "source": [
    "def plot_origin_img_bbox(img_id, img_path, target):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # plot the image and bboxes\n",
    "    # Bounding boxes are defined as follows: x-min y-min width height\n",
    "    fig, a = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(20, 20)\n",
    "    plt.imshow(img)\n",
    "    currentAxis=plt.gca()\n",
    "    for box in (target['boxes']):\n",
    "        x, y = box[0] * (img.shape[1] / IMAGE_SIZE), box[1] * (img.shape[0] / IMAGE_SIZE)\n",
    "        width, height = (box[2] - box[0]) * (img.shape[1] / IMAGE_SIZE), (box[3] - box[1]) * (img.shape[0] / IMAGE_SIZE)\n",
    "        padding_width, padding_height = width * 0.1, height * 0.1\n",
    "        rect = patches.Rectangle((x, y),\n",
    "                                 width, height,\n",
    "                                 linewidth=1,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none'\n",
    "                                 )\n",
    "        # Draw the bounding box on top of the image\n",
    "        currentAxis.add_patch(rect)\n",
    "\n",
    "    plt.savefig(str(int(time.time()))+str(random.randint(0,100))+\".png\", dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1645946176101,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "Ls7oCRfuCM2Y"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255740,
     "status": "ok",
     "timestamp": 1645946659657,
     "user": {
      "displayName": "zy one",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfFtZjlpPfcMMkWKhk6ZmQ5A5tIxO-4cwOGCzN=s64",
      "userId": "13839058139856352710"
     },
     "user_tz": -480
    },
    "id": "ecSnsWMT7iT-",
    "outputId": "d3341564-d5f3-4717-bf05-ac51a4801827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores: 0.9981048387096775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2  # 1 class (wheat) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/models/fasterrcnn_resnet50_fpn_epoch_3.pth'))\n",
    "model.eval()\n",
    "x = model.to(device)\n",
    "\n",
    "test_dataset = TestTextDataset(valid_df, get_test_transform())\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    "\n",
    ")\n",
    "detection_threshold = 0.5\n",
    "results = []\n",
    "score_list = []\n",
    "for images, image_ids, image_paths in test_data_loader:\n",
    "\n",
    "    images = list(image.to(device) for image in images)\n",
    "    outputs = model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "\n",
    "        score_list.append(np.mean(scores).tolist())\n",
    "\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "\n",
    "        image_id = image_ids[i]\n",
    "        path = image_paths[i]\n",
    "\n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores),\n",
    "            'path':path\n",
    "\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "avg_score = 0.0\n",
    "c = 0\n",
    "for i in score_list:\n",
    "    try:\n",
    "        avg_score += int(i * 1000)\n",
    "    except:\n",
    "        c += 1\n",
    "avg_score = avg_score / 1000\n",
    "print(\"Average scores:\", avg_score / (len(score_list) - c))\n",
    "\n",
    "test_target = []\n",
    "for t in results:\n",
    "    tg = {}\n",
    "    tg['image_id'] = t['image_id']\n",
    "    tg['image_path'] = t['path']\n",
    "    boxes = np.array(([list(map(int, b.split()[1:])) for b in t['PredictionString']]))\n",
    "    boxes = np.reshape(boxes, (len(boxes), 4))\n",
    "    tg['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    tg['scores'] = torch.as_tensor([float(list(b.split(' '))[0]) for b in t['PredictionString']],\n",
    "                                    dtype=torch.float32)\n",
    "    tg['labels'] = torch.ones((tg['boxes'].shape[0],), dtype=torch.int64)\n",
    "    test_target.append(tg)\n",
    "\n",
    "for i in test_target:\n",
    "    nms_prediction =apply_nms(i,iou_thresh=0.2)\n",
    "    plot_origin_img_bbox(img_id=i['image_id'],img_path=i['image_path'],target=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSqXxL8v7OBL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNCCTnoGC4BPMgCTUFfuDeb",
   "machine_shape": "hm",
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "limuPytorch",
   "language": "python",
   "name": "limupytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
